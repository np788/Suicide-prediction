{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import sample\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import brier_score_loss, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc, accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "B = 501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SCI.csv\")\n",
    "df = df.drop('StudyID', axis=1)\n",
    "df = df.rename(columns={'FU_SA': 'Outcome'})\n",
    "y = df.Outcome\n",
    "X = df.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df = pd.read_csv(\"nd.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "df['Outcome'] = df['3Actual _Attempt'] + df['3Interrupted_ attempt'] + df['3Aborted _Attempt']\n",
    "df = df[df['Outcome'].notna()]\n",
    "df = df.drop(['Study Id', 'age', 'annual income', 'marital stat', 'lives with', 'gender', 'race', 'ethnicity', 'yrs of edu', '3CSSRS-_most severe_ ideation', '3Actual _Attempt', '3Interrupted_ attempt', '3Aborted _Attempt'], axis=1)\n",
    "df['Outcome'] = df['Outcome'].astype(bool).astype(int)\n",
    "df=df.dropna(axis=0,how='any')\n",
    "df=df.dropna(axis=1,how='any')\n",
    "#print(df['Outcome'].value_counts())\n",
    "\n",
    "y = df.Outcome\n",
    "X = df.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.random.randint(0,5,(591,49))\n",
    "X = pd.DataFrame.from_records(x)\n",
    "y = pd.DataFrame.from_records(np.random.randint(0,1,(571,1)))\n",
    "y2 = pd.DataFrame.from_records(np.random.randint(1,2,(20,1)))\n",
    "y3 = pd.concat([y, y2])\n",
    "y3['index'] = np.arange(len(y3))\n",
    "y3 = y3.set_index('index')\n",
    "y3 = y3.reindex(np.random.permutation(y3.index))\n",
    "y3.columns = ['Outcome']\n",
    "df = pd.concat([X, y3], axis = 1)\n",
    "y = y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate algos and fit data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X,y)\n",
    "\n",
    "gb = XGBClassifier()\n",
    "gb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find APPARENT predictions\n",
    "\n",
    "#LR\n",
    "preds_app_lr = lr.predict(X)\n",
    "probs_app_lr = lr.predict_proba(X)[:,1]\n",
    "\n",
    "#RF\n",
    "preds_app_rf = rf.predict(X)\n",
    "probs_app_rf = rf.predict_proba(X)[:,1]\n",
    "\n",
    "#GB\n",
    "preds_app_gb = gb.predict(X)\n",
    "probs_app_gb = gb.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine APPARENT metrics\n",
    "\n",
    "#LR\n",
    "acc_app_lr = accuracy_score(y, preds_app_lr)\n",
    "bacc_app_lr = balanced_accuracy_score(y, preds_app_lr)\n",
    "roc_app_lr = roc_auc_score(y, probs_app_lr)\n",
    "bs_app_lr = brier_score_loss(y, probs_app_lr)\n",
    "prec_app_lr = precision_score(y, preds_app_lr)\n",
    "rec_app_lr = recall_score(y, preds_app_lr)\n",
    "\n",
    "prec_app_lr2, rec_app_lr2, _ = precision_recall_curve(y, probs_app_lr)\n",
    "prc_app_lr = auc(rec_app_lr2, prec_app_lr2)\n",
    "\n",
    "#RF\n",
    "acc_app_rf = accuracy_score(y, preds_app_rf)\n",
    "bacc_app_rf = balanced_accuracy_score(y, preds_app_rf)\n",
    "roc_app_rf = roc_auc_score(y, probs_app_rf)\n",
    "bs_app_rf = brier_score_loss(y, probs_app_rf)\n",
    "prec_app_rf = precision_score(y, preds_app_rf)\n",
    "rec_app_rf = recall_score(y, preds_app_rf)\n",
    "\n",
    "prec_app_rf2, rec_app_rf2, _ = precision_recall_curve(y, probs_app_rf)\n",
    "prc_app_rf = auc(rec_app_rf2, prec_app_rf2)\n",
    "\n",
    "#GB\n",
    "acc_app_gb = accuracy_score(y, preds_app_gb)\n",
    "bacc_app_gb = balanced_accuracy_score(y, preds_app_gb)\n",
    "roc_app_gb = roc_auc_score(y, probs_app_gb)\n",
    "bs_app_gb = brier_score_loss(y, probs_app_gb)\n",
    "prec_app_gb = precision_score(y, preds_app_gb)\n",
    "rec_app_gb = recall_score(y, preds_app_gb)\n",
    "\n",
    "prec_app_gb2, rec_app_gb2, _ = precision_recall_curve(y, probs_app_gb)\n",
    "prc_app_gb = auc(rec_app_gb2, prec_app_gb2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create arrays for optimism values\n",
    "\n",
    "#LR\n",
    "acc_lr = []\n",
    "bacc_lr = []\n",
    "roc_lr = []\n",
    "bs_lr = []\n",
    "prec_lr = []\n",
    "rec_lr = []\n",
    "prc_lr = []\n",
    "\n",
    "#RF\n",
    "acc_rf = []\n",
    "bacc_rf = []\n",
    "roc_rf = []\n",
    "bs_rf = []\n",
    "prec_rf = []\n",
    "rec_rf = []\n",
    "prc_rf = []\n",
    "\n",
    "#GB\n",
    "acc_gb = []\n",
    "bacc_gb = []\n",
    "roc_gb = []\n",
    "bs_gb = []\n",
    "prec_gb = []\n",
    "rec_gb = []\n",
    "prc_gb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, B):\n",
    "    #Create bootstrap sample, fit algo, make predictions\n",
    "    boot = resample(df, replace=True, n_samples=len(X)) #Create bootstrapped dataset\n",
    "    y_b = boot.Outcome \n",
    "    X_b = boot.drop('Outcome', axis=1)\n",
    "\n",
    "\n",
    "    lr.fit(X_b, y_b) #Fit LR using boostrapped data    \n",
    "    pred_b_lr = lr.predict(X_b)\n",
    "    prob_b_lr = lr.predict_proba(X_b)[:,1]\n",
    "    pred_o_lr = lr.predict(X)\n",
    "    prob_o_lr = lr.predict_proba(X)[:,1]\n",
    "\n",
    "    rf.fit(X_b, y_b) #Fit rf using boostrapped data    \n",
    "    pred_b_rf = rf.predict(X_b)\n",
    "    prob_b_rf = rf.predict_proba(X_b)[:,1]\n",
    "    pred_o_rf = rf.predict(X)\n",
    "    prob_o_rf = rf.predict_proba(X)[:,1]\n",
    "\n",
    "    gb.fit(X_b, y_b) #Fit gb using boostrapped data    \n",
    "    pred_b_gb = gb.predict(X_b)\n",
    "    prob_b_gb = gb.predict_proba(X_b)[:,1]\n",
    "    pred_o_gb = gb.predict(X)\n",
    "    prob_o_gb = gb.predict_proba(X)[:,1]\n",
    "    \n",
    "    \n",
    "    #Accuracy\n",
    "    acc_b_lr = accuracy_score(y_b, pred_b_lr)\n",
    "    acc_o_lr = accuracy_score(y, pred_o_lr)\n",
    "    acc_lr.append(acc_b_lr-acc_o_lr)\n",
    "    \n",
    "    acc_b_rf = accuracy_score(y_b, pred_b_rf)\n",
    "    acc_o_rf = accuracy_score(y, pred_o_rf)\n",
    "    acc_rf.append(acc_b_rf-acc_o_rf)\n",
    "    \n",
    "    acc_b_gb = accuracy_score(y_b, pred_b_gb)\n",
    "    acc_o_gb = accuracy_score(y, pred_o_gb)\n",
    "    acc_gb.append(acc_b_gb-acc_o_gb)    \n",
    "    \n",
    "    #Balanced accuracy\n",
    "    bacc_b_lr = balanced_accuracy_score(y_b, pred_b_lr)\n",
    "    bacc_o_lr = balanced_accuracy_score(y, pred_o_lr)\n",
    "    bacc_lr.append(bacc_b_lr - bacc_o_lr)\n",
    "    \n",
    "    bacc_b_rf = balanced_accuracy_score(y_b, pred_b_rf)\n",
    "    bacc_o_rf = balanced_accuracy_score(y, pred_o_rf)\n",
    "    bacc_rf.append(bacc_b_rf - bacc_o_rf)\n",
    "    \n",
    "    bacc_b_gb = balanced_accuracy_score(y_b, pred_b_gb)\n",
    "    bacc_o_gb = balanced_accuracy_score(y, pred_o_gb)\n",
    "    bacc_gb.append(bacc_b_gb - bacc_o_gb)\n",
    "    \n",
    "    #AUROC\n",
    "    roc_b_lr = roc_auc_score(y_b, prob_b_lr)\n",
    "    roc_o_lr = roc_auc_score(y, prob_o_lr)\n",
    "    roc_lr.append(roc_b_lr-roc_o_lr)\n",
    "    \n",
    "    roc_b_rf = roc_auc_score(y_b, prob_b_rf)\n",
    "    roc_o_rf = roc_auc_score(y, prob_o_rf)\n",
    "    roc_rf.append(roc_b_rf-roc_o_rf)    \n",
    "\n",
    "    roc_b_gb = roc_auc_score(y_b, prob_b_gb)\n",
    "    roc_o_gb = roc_auc_score(y, prob_o_gb)\n",
    "    roc_gb.append(roc_b_gb-roc_o_gb)    \n",
    "    \n",
    "    #Brier\n",
    "    bs_b_lr = brier_score_loss(y_b, prob_b_lr)\n",
    "    bs_o_lr = brier_score_loss(y, prob_o_lr)\n",
    "    bs_lr.append(bs_b_lr - bs_o_lr)\n",
    "\n",
    "    bs_b_rf = brier_score_loss(y_b, prob_b_rf)\n",
    "    bs_o_rf = brier_score_loss(y, prob_o_rf)\n",
    "    bs_rf.append(bs_b_rf - bs_o_rf)    \n",
    "    \n",
    "    bs_b_gb = brier_score_loss(y_b, prob_b_gb)\n",
    "    bs_o_gb = brier_score_loss(y, prob_o_gb)\n",
    "    bs_gb.append(bs_b_gb - bs_o_gb)    \n",
    "    \n",
    "    #Precision\n",
    "    prec_b_lr = precision_score(y_b, pred_b_lr)\n",
    "    prec_o_lr = precision_score(y, pred_o_lr)\n",
    "    prec_lr.append(prec_b_lr - prec_o_lr)\n",
    "\n",
    "    prec_b_rf = precision_score(y_b, pred_b_rf)\n",
    "    prec_o_rf = precision_score(y, pred_o_rf)\n",
    "    prec_rf.append(prec_b_rf - prec_o_rf)    \n",
    "\n",
    "    prec_b_gb = precision_score(y_b, pred_b_gb)\n",
    "    prec_o_gb = precision_score(y, pred_o_gb)\n",
    "    prec_gb.append(prec_b_gb - prec_o_gb)    \n",
    "    \n",
    "    #Recall\n",
    "    rec_b_lr = recall_score(y_b, pred_b_lr)\n",
    "    rec_o_lr = recall_score(y, pred_o_lr)\n",
    "    rec_lr.append(rec_b_lr - rec_o_lr)\n",
    "\n",
    "    rec_b_rf = recall_score(y_b, pred_b_rf)\n",
    "    rec_o_rf = recall_score(y, pred_o_rf)\n",
    "    rec_rf.append(rec_b_rf - rec_o_rf)    \n",
    "\n",
    "    rec_b_gb = recall_score(y_b, pred_b_gb)\n",
    "    rec_o_gb = recall_score(y, pred_o_gb)\n",
    "    rec_gb.append(rec_b_gb - rec_o_gb)    \n",
    "    \n",
    "    #PRC\n",
    "    prec_b_lr2, rec_b_lr2, _ = precision_recall_curve(y_b, prob_b_lr)\n",
    "    prc_b_lr = auc(rec_b_lr2, prec_b_lr2)\n",
    "\n",
    "    prec_o_lr2, rec_o_lr2, _ = precision_recall_curve(y, prob_o_lr)\n",
    "    prc_o_lr = auc(rec_o_lr2, prec_o_lr2)\n",
    "    \n",
    "    prc_lr.append(prc_b_lr-prc_o_lr)\n",
    "    \n",
    "    prec_b_rf2, rec_b_rf2, _ = precision_recall_curve(y_b, prob_b_rf)\n",
    "    prc_b_rf = auc(rec_b_rf2, prec_b_rf2)\n",
    "\n",
    "    prec_o_rf2, rec_o_rf2, _ = precision_recall_curve(y, prob_o_rf)\n",
    "    prc_o_rf = auc(rec_o_rf2, prec_o_rf2)\n",
    "    \n",
    "    prc_rf.append(prc_b_rf-prc_o_rf)    \n",
    "\n",
    "    prec_b_gb2, rec_b_gb2, _ = precision_recall_curve(y_b, prob_b_gb)\n",
    "    prc_b_gb = auc(rec_b_gb2, prec_b_gb2)\n",
    "\n",
    "    prec_o_gb2, rec_o_gb2, _ = precision_recall_curve(y, prob_o_gb)\n",
    "    prc_o_gb = auc(rec_o_gb2, prec_o_gb2)\n",
    "    \n",
    "    prc_gb.append(prc_b_gb-prc_o_gb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Adjusted AUROC 0.8290024712862887\n",
      "Adjusted accuracy 0.9618341793570221\n",
      "Adjusted balanced accuracy 0.5705317309100856\n",
      "Adjusted precision 0.6092913758344725\n",
      "Adjusted recall 0.15082820188203602\n",
      "Adjusted AUPRC 0.1258640666555646\n",
      "Adjusted Brier score 0.03526684444561276\n",
      "\n",
      "\n",
      "RF\n",
      "Adjusted AUROC 0.921333887915937\n",
      "Adjusted accuracy 0.9873874788494078\n",
      "Adjusted balanced accuracy 0.8152017663151006\n",
      "Adjusted precision 0.9955719738208129\n",
      "Adjusted recall 0.6305156166932482\n",
      "Adjusted AUPRC 0.7382087192478024\n",
      "Adjusted Brier score 0.017484746531302876\n",
      "\n",
      "\n",
      "GB\n",
      "Adjusted AUROC 0.8957535026269703\n",
      "Adjusted accuracy 0.9857157360406091\n",
      "Adjusted balanced accuracy 0.8165965504563338\n",
      "Adjusted precision 0.9264248074337084\n",
      "Adjusted recall 0.635168582523876\n",
      "Adjusted AUPRC 0.7161010302112542\n",
      "Adjusted Brier score 0.013890580195884435\n"
     ]
    }
   ],
   "source": [
    "# Calculate optimisms and adjusted results\n",
    "\n",
    "# Accuracy\n",
    "acc_opt_lr = np.mean(acc_lr)\n",
    "acc_adj_lr = acc_app_lr - acc_opt_lr\n",
    "\n",
    "acc_opt_rf = np.mean(acc_rf)\n",
    "acc_adj_rf = acc_app_rf - acc_opt_rf\n",
    "\n",
    "acc_opt_gb = np.mean(acc_gb)\n",
    "acc_adj_gb = acc_app_gb - acc_opt_gb\n",
    "\n",
    "# Balanced accuracy\n",
    "bacc_opt_lr = np.mean(bacc_lr)\n",
    "bacc_adj_lr = bacc_app_lr - bacc_opt_lr\n",
    "\n",
    "bacc_opt_rf = np.mean(bacc_rf)\n",
    "bacc_adj_rf = bacc_app_rf - bacc_opt_rf\n",
    "\n",
    "bacc_opt_gb = np.mean(bacc_gb)\n",
    "bacc_adj_gb = bacc_app_gb - bacc_opt_gb\n",
    "\n",
    "# AUROC\n",
    "roc_opt_lr = np.mean(roc_lr)\n",
    "roc_adj_lr = roc_app_lr - roc_opt_lr\n",
    "\n",
    "roc_opt_rf = np.mean(roc_rf)\n",
    "roc_adj_rf = roc_app_rf - roc_opt_rf\n",
    "\n",
    "roc_opt_gb = np.mean(roc_gb)\n",
    "roc_adj_gb = roc_app_gb - roc_opt_gb\n",
    "\n",
    "# Precision\n",
    "prec_opt_lr = np.mean(prec_lr)\n",
    "prec_adj_lr = prec_app_lr - prec_opt_lr\n",
    "\n",
    "prec_opt_rf = np.mean(prec_rf)\n",
    "prec_adj_rf = prec_app_rf - prec_opt_rf\n",
    "\n",
    "prec_opt_gb = np.mean(prec_gb)\n",
    "prec_adj_gb = prec_app_gb - prec_opt_gb\n",
    "\n",
    "# Recall\n",
    "rec_opt_lr = np.mean(rec_lr)\n",
    "rec_adj_lr = rec_app_lr - rec_opt_lr\n",
    "\n",
    "rec_opt_rf = np.mean(rec_rf)\n",
    "rec_adj_rf = rec_app_rf - rec_opt_rf\n",
    "\n",
    "rec_opt_gb = np.mean(rec_gb)\n",
    "rec_adj_gb = rec_app_gb - rec_opt_gb\n",
    "\n",
    "# Brier score\n",
    "bs_opt_lr = np.mean(bs_lr)\n",
    "bs_adj_lr = bs_app_lr - bs_opt_lr\n",
    "\n",
    "bs_opt_rf = np.mean(bs_rf)\n",
    "bs_adj_rf = bs_app_rf - bs_opt_rf\n",
    "\n",
    "bs_opt_gb = np.mean(bs_gb)\n",
    "bs_adj_gb = bs_app_gb - bs_opt_gb\n",
    "\n",
    "# AUPRC\n",
    "prc_opt_lr = np.mean(prc_lr)\n",
    "prc_adj_lr = prc_app_lr - prc_opt_lr\n",
    "\n",
    "prc_opt_rf = np.mean(prc_rf)\n",
    "prc_adj_rf = prc_app_rf - prc_opt_rf\n",
    "\n",
    "prc_opt_gb = np.mean(prc_gb)\n",
    "prc_adj_gb = prc_app_gb - prc_opt_gb\n",
    "\n",
    "print('LR')\n",
    "print('Adjusted AUROC', roc_adj_lr)#, '; Optimism', roc_opt_lr)\n",
    "print('Adjusted accuracy', acc_adj_lr)#, '; Optimism', acc_opt_lr)\n",
    "print('Adjusted balanced accuracy', bacc_adj_lr)#, '; Optimism', bacc_opt_lr)\n",
    "print('Adjusted precision', prec_adj_lr)#, '; Optimism', prec_opt_lr)\n",
    "print('Adjusted recall', rec_adj_lr)#, '; Optimism', rec_opt_lr)\n",
    "print('Adjusted AUPRC', prc_adj_lr)#, '; Optimism', prc_opt_lr)\n",
    "print('Adjusted Brier score', bs_adj_lr)#, '; Optimism', bs_opt_lr)\n",
    "print('\\n')\n",
    "\n",
    "print('RF')\n",
    "print('Adjusted AUROC', roc_adj_rf)#, '; Optimism', roc_opt_rf)\n",
    "print('Adjusted accuracy', acc_adj_rf)#, '; Optimism', acc_opt_rf)\n",
    "print('Adjusted balanced accuracy', bacc_adj_rf)#, '; Optimism', bacc_opt_rf)\n",
    "print('Adjusted precision', prec_adj_rf)#, '; Optimism', prec_opt_rf)\n",
    "print('Adjusted recall', rec_adj_rf)#, '; Optimism', rec_opt_rf)\n",
    "print('Adjusted AUPRC', prc_adj_rf)#, '; Optimism', prc_opt_rf)\n",
    "print('Adjusted Brier score', bs_adj_rf)#, '; Optimism', bs_opt_rf)\n",
    "print('\\n')\n",
    "\n",
    "print('GB')\n",
    "print('Adjusted AUROC', roc_adj_gb)#, '; Optimism', roc_opt_gb)\n",
    "print('Adjusted accuracy', acc_adj_gb)#, '; Optimism', acc_opt_gb)\n",
    "print('Adjusted balanced accuracy', bacc_adj_gb)#, '; Optimism', bacc_opt_gb)\n",
    "print('Adjusted precision', prec_adj_gb)#, '; Optimism', prec_opt_gb)\n",
    "print('Adjusted recall', rec_adj_gb)#, '; Optimism', rec_opt_gb)\n",
    "print('Adjusted AUPRC', prc_adj_gb)#, '; Optimism', prc_opt_gb)\n",
    "print('Adjusted Brier score', bs_adj_gb)#, '; Optimism', bs_opt_gb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
